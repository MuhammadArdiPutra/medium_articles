{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3de3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 1\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37676ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 2\n",
    "BATCH_SIZE   = 1      #(1)\n",
    "\n",
    "IMAGE_SIZE   = 384    #(2)\n",
    "IN_CHANNELS  = 3      #(3)\n",
    "\n",
    "SEQ_LENGTH   = 30     #(4)\n",
    "VOCAB_SIZE   = 10000  #(5)\n",
    "\n",
    "EMBED_DIM          = 768  #(6)\n",
    "PATCH_SIZE         = 16   #(7)\n",
    "NUM_PATCHES        = (IMAGE_SIZE//PATCH_SIZE) ** 2  #(8)\n",
    "NUM_ENCODER_BLOCKS = 12   #(9)\n",
    "NUM_DECODER_BLOCKS = 4    #(10)\n",
    "NUM_HEADS          = 12   #(11)\n",
    "HIDDEN_DIM         = EMBED_DIM * 4  #(12)\n",
    "DROP_PROB          = 0.1  #(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06af30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 3\n",
    "class Patcher(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #(1)\n",
    "        self.unfold = nn.Unfold(kernel_size=PATCH_SIZE, stride=PATCH_SIZE)\n",
    "\n",
    "        #(2)\n",
    "        self.linear_projection = nn.Linear(in_features=IN_CHANNELS*PATCH_SIZE*PATCH_SIZE, \n",
    "                                           out_features=EMBED_DIM)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        #print(f'images\\t\\t: {images.size()}')\n",
    "        \n",
    "        images = self.unfold(images)  #(3)\n",
    "        #print(f'after unfold\\t: {images.size()}')\n",
    "        \n",
    "        images = images.permute(0, 2, 1)  #(4)\n",
    "        #print(f'after permute\\t: {images.size()}')\n",
    "        \n",
    "        features = self.linear_projection(images)  #(5)\n",
    "        #print(f'after lin proj\\t: {features.size()}')\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c12617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\t\t: torch.Size([1, 3, 384, 384])\n",
      "after unfold\t: torch.Size([1, 768, 576])\n",
      "after permute\t: torch.Size([1, 576, 768])\n",
      "after lin proj\t: torch.Size([1, 576, 768])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 4\n",
    "patcher  = Patcher()\n",
    "\n",
    "images   = torch.randn(BATCH_SIZE, IN_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "features = patcher(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "593adcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 5\n",
    "class LearnableEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.learnable_embedding = nn.Parameter(torch.randn(size=(NUM_PATCHES, EMBED_DIM)), \n",
    "                                                requires_grad=True)\n",
    "        \n",
    "    def forward(self):\n",
    "        pos_embed = self.learnable_embedding\n",
    "        #print(f'learnable embedding\\t: {pos_embed.size()}')\n",
    "        \n",
    "        return pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3286b306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learnable embedding\t: torch.Size([576, 768])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 6\n",
    "learnable_embedding = LearnableEmbedding()\n",
    "\n",
    "pos_embed = learnable_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6528a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 7a\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #(1)\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=EMBED_DIM,\n",
    "                                                    num_heads=NUM_HEADS, \n",
    "                                                    batch_first=True,  #(2)\n",
    "                                                    dropout=DROP_PROB)\n",
    "        \n",
    "        self.layer_norm_0 = nn.LayerNorm(EMBED_DIM)  #(3)\n",
    "        \n",
    "        self.ffn = nn.Sequential(  #(4)\n",
    "            nn.Linear(in_features=EMBED_DIM, out_features=HIDDEN_DIM),\n",
    "            nn.GELU(), \n",
    "            nn.Dropout(p=DROP_PROB), \n",
    "            nn.Linear(in_features=HIDDEN_DIM, out_features=EMBED_DIM),\n",
    "        )\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm(EMBED_DIM)  #(5)\n",
    "        \n",
    "# Codeblock 7b\n",
    "    def forward(self, features):  #(1)\n",
    "        \n",
    "        residual = features  #(2)\n",
    "        #print(f'features & residual\\t: {residual.size()}')\n",
    "        \n",
    "        #(3)\n",
    "        features, self_attn_weights = self.self_attention(query=features, \n",
    "                                                          key=features, \n",
    "                                                          value=features)\n",
    "        #print(f'after self attention\\t: {features.size()}')\n",
    "        #print(f\"self attn weights\\t: {self_attn_weights.shape}\")\n",
    "        \n",
    "        features = self.layer_norm_0(features + residual)  #(4)\n",
    "        #print(f'after norm\\t\\t: {features.size()}')\n",
    "        \n",
    "\n",
    "        residual = features\n",
    "        #print(f'\\nfeatures & residual\\t: {residual.size()}')\n",
    "        \n",
    "        features = self.ffn(features)  #(5)\n",
    "        #print(f'after ffn\\t\\t: {features.size()}')\n",
    "        \n",
    "        features = self.layer_norm_1(features + residual)\n",
    "        #print(f'after norm\\t\\t: {features.size()}')\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ecdcf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features & residual\t: torch.Size([1, 576, 768])\n",
      "after self attention\t: torch.Size([1, 576, 768])\n",
      "self attn weights\t: torch.Size([1, 576, 576])\n",
      "after norm\t\t: torch.Size([1, 576, 768])\n",
      "\n",
      "features & residual\t: torch.Size([1, 576, 768])\n",
      "after ffn\t\t: torch.Size([1, 576, 768])\n",
      "after norm\t\t: torch.Size([1, 576, 768])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 8\n",
    "encoder_block = EncoderBlock()\n",
    "\n",
    "features = torch.randn(BATCH_SIZE, NUM_PATCHES, EMBED_DIM)\n",
    "features = encoder_block(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef4e79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 9\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patcher = Patcher()  #(1)\n",
    "        self.learnable_embedding = LearnableEmbedding()  #(2)\n",
    "\n",
    "        #(3)\n",
    "        self.encoder_blocks = nn.ModuleList(EncoderBlock() for _ in range(NUM_ENCODER_BLOCKS))\n",
    "    \n",
    "    def forward(self, images):  #(4)\n",
    "        #print(f'images\\t\\t\\t: {images.size()}')\n",
    "        \n",
    "        features = self.patcher(images)  #(5)\n",
    "        #print(f'after patcher\\t\\t: {features.size()}')\n",
    "        \n",
    "        features = features + self.learnable_embedding()  #(6)\n",
    "        #print(f'after learn embed\\t: {features.size()}')\n",
    "        \n",
    "        for i, encoder_block in enumerate(self.encoder_blocks):\n",
    "            features = encoder_block(features)  #(7)\n",
    "            #print(f\"after encoder block #{i}\\t: {features.shape}\")\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a802fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\t\t\t: torch.Size([1, 3, 384, 384])\n",
      "after patcher\t\t: torch.Size([1, 576, 768])\n",
      "after learn embed\t: torch.Size([1, 576, 768])\n",
      "after encoder block #0\t: torch.Size([1, 576, 768])\n",
      "after encoder block #1\t: torch.Size([1, 576, 768])\n",
      "after encoder block #2\t: torch.Size([1, 576, 768])\n",
      "after encoder block #3\t: torch.Size([1, 576, 768])\n",
      "after encoder block #4\t: torch.Size([1, 576, 768])\n",
      "after encoder block #5\t: torch.Size([1, 576, 768])\n",
      "after encoder block #6\t: torch.Size([1, 576, 768])\n",
      "after encoder block #7\t: torch.Size([1, 576, 768])\n",
      "after encoder block #8\t: torch.Size([1, 576, 768])\n",
      "after encoder block #9\t: torch.Size([1, 576, 768])\n",
      "after encoder block #10\t: torch.Size([1, 576, 768])\n",
      "after encoder block #11\t: torch.Size([1, 576, 768])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 10\n",
    "encoder = Encoder()\n",
    "\n",
    "images = torch.randn(BATCH_SIZE, IN_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "features = encoder(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b2effa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 11\n",
    "class EncoderTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patcher = Patcher()\n",
    "        self.learnable_embedding = LearnableEmbedding()\n",
    "        \n",
    "        #(1)\n",
    "        encoder_block = nn.TransformerEncoderLayer(d_model=EMBED_DIM, \n",
    "                                                   nhead=NUM_HEADS, \n",
    "                                                   dim_feedforward=HIDDEN_DIM, \n",
    "                                                   dropout=DROP_PROB, \n",
    "                                                   batch_first=True)\n",
    "        \n",
    "        #(2)\n",
    "        self.encoder_blocks = nn.TransformerEncoder(encoder_layer=encoder_block, \n",
    "                                                    num_layers=NUM_ENCODER_BLOCKS)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        #print(f'images\\t\\t\\t: {images.size()}')\n",
    "        \n",
    "        features = self.patcher(images)\n",
    "        #print(f'after patcher\\t\\t: {features.size()}')\n",
    "        \n",
    "        features = features + self.learnable_embedding()\n",
    "        #print(f'after learn embed\\t: {features.size()}')\n",
    "        \n",
    "        features = self.encoder_blocks(features)  #(3)\n",
    "        #print(f'after encoder blocks\\t: {features.size()}')\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71d81e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\t\t\t: torch.Size([1, 3, 384, 384])\n",
      "after patcher\t\t: torch.Size([1, 576, 768])\n",
      "after learn embed\t: torch.Size([1, 576, 768])\n",
      "after encoder blocks\t: torch.Size([1, 576, 768])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 12\n",
    "encoder_torch = EncoderTorch()\n",
    "\n",
    "images = torch.randn(BATCH_SIZE, IN_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "features = encoder_torch(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64bcb6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 13\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def forward(self):\n",
    "        pos = torch.arange(SEQ_LENGTH).reshape(SEQ_LENGTH, 1)\n",
    "        #print(f\"pos\\t\\t: {pos.shape}\")\n",
    "        \n",
    "        i = torch.arange(0, EMBED_DIM, 2)\n",
    "        denominator = torch.pow(10000, i/EMBED_DIM)\n",
    "        #print(f\"denominator\\t: {denominator.shape}\")\n",
    "        \n",
    "        even_pos_embed = torch.sin(pos/denominator)  #(1)\n",
    "        odd_pos_embed  = torch.cos(pos/denominator)  #(2)\n",
    "        #print(f\"even_pos_embed\\t: {even_pos_embed.shape}\")\n",
    "        \n",
    "        stacked = torch.stack([even_pos_embed, odd_pos_embed], dim=2)  #(3)\n",
    "        #print(f\"stacked\\t\\t: {stacked.shape}\")\n",
    "\n",
    "        pos_embed = torch.flatten(stacked, start_dim=1, end_dim=2)  #(4)\n",
    "        #print(f\"pos_embed\\t: {pos_embed.shape}\")\n",
    "        \n",
    "        return pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "129739db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\t\t: torch.Size([30, 1])\n",
      "denominator\t: torch.Size([384])\n",
      "even_pos_embed\t: torch.Size([30, 384])\n",
      "stacked\t\t: torch.Size([30, 384, 2])\n",
      "pos_embed\t: torch.Size([30, 768])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 14\n",
    "sinusoidal_embedding = SinusoidalEmbedding()\n",
    "pos_embed = sinusoidal_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0f555c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 15\n",
    "def create_mask(seq_length):\n",
    "    mask = torch.tril(torch.ones((seq_length, seq_length)))  #(1)\n",
    "    mask[mask == 0] = -float('inf')  #(2)\n",
    "    mask[mask == 1] = 0  #(3)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "029f16de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codeblock 16\n",
    "mask_example = create_mask(seq_length=7)\n",
    "mask_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9338939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 17a\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #(1)\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=EMBED_DIM, \n",
    "                                                    num_heads=NUM_HEADS, \n",
    "                                                    batch_first=True, \n",
    "                                                    dropout=DROP_PROB)\n",
    "        #(2)\n",
    "        self.layer_norm_0 = nn.LayerNorm(EMBED_DIM)\n",
    "        \n",
    "        #(3)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=EMBED_DIM, \n",
    "                                                     num_heads=NUM_HEADS, \n",
    "                                                     batch_first=True, \n",
    "                                                     dropout=DROP_PROB)\n",
    "\n",
    "        #(4)\n",
    "        self.layer_norm_1 = nn.LayerNorm(EMBED_DIM)\n",
    "        \n",
    "        #(5)       \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=EMBED_DIM, out_features=HIDDEN_DIM),\n",
    "            nn.GELU(), \n",
    "            nn.Dropout(p=DROP_PROB), \n",
    "            nn.Linear(in_features=HIDDEN_DIM, out_features=EMBED_DIM),\n",
    "        )\n",
    "        \n",
    "        #(6)\n",
    "        self.layer_norm_2 = nn.LayerNorm(EMBED_DIM)\n",
    "        \n",
    "# Codeblock 17b\n",
    "    def forward(self, features, captions, attn_mask):  #(1)\n",
    "        #print(f\"attn_mask\\t\\t: {attn_mask.shape}\")\n",
    "        residual = captions\n",
    "        #print(f\"captions & residual\\t: {captions.shape}\")\n",
    "        \n",
    "        #(2)\n",
    "        captions, self_attn_weights = self.self_attention(query=captions, \n",
    "                                                          key=captions, \n",
    "                                                          value=captions, \n",
    "                                                          attn_mask=attn_mask)\n",
    "        #print(f\"after self attention\\t: {captions.shape}\")\n",
    "        #print(f\"self attn weights\\t: {self_attn_weights.shape}\")\n",
    "        \n",
    "        captions = self.layer_norm_0(captions + residual)\n",
    "        #print(f\"after norm\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        \n",
    "        #print(f\"\\nfeatures\\t\\t: {features.shape}\")\n",
    "        residual = captions\n",
    "        #print(f\"captions & residual\\t: {captions.shape}\")\n",
    "        \n",
    "        #(3)\n",
    "        captions, cross_attn_weights = self.cross_attention(query=captions, \n",
    "                                                            key=features, \n",
    "                                                            value=features)\n",
    "        #print(f\"after cross attention\\t: {captions.shape}\")\n",
    "        #print(f\"cross attn weights\\t: {cross_attn_weights.shape}\")\n",
    "        \n",
    "        captions = self.layer_norm_1(captions + residual)\n",
    "        #print(f\"after norm\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        residual = captions\n",
    "        #print(f\"\\ncaptions & residual\\t: {captions.shape}\")\n",
    "        \n",
    "        captions = self.ffn(captions)  #(4)\n",
    "        #print(f\"after ffn\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        captions = self.layer_norm_2(captions + residual)\n",
    "        #print(f\"after norm\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2ecaef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_mask\t\t: torch.Size([30, 30])\n",
      "captions & residual\t: torch.Size([1, 30, 768])\n",
      "after self attention\t: torch.Size([1, 30, 768])\n",
      "self attn weights\t: torch.Size([1, 30, 30])\n",
      "after norm\t\t: torch.Size([1, 30, 768])\n",
      "\n",
      "features\t\t: torch.Size([1, 576, 768])\n",
      "captions & residual\t: torch.Size([1, 30, 768])\n",
      "after cross attention\t: torch.Size([1, 30, 768])\n",
      "cross attn weights\t: torch.Size([1, 30, 576])\n",
      "after norm\t\t: torch.Size([1, 30, 768])\n",
      "\n",
      "captions & residual\t: torch.Size([1, 30, 768])\n",
      "after ffn\t\t: torch.Size([1, 30, 768])\n",
      "after norm\t\t: torch.Size([1, 30, 768])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 18\n",
    "decoder_block = DecoderBlock()\n",
    "\n",
    "features = torch.randn(BATCH_SIZE, NUM_PATCHES, EMBED_DIM)  #(1)\n",
    "captions = torch.randn(BATCH_SIZE, SEQ_LENGTH, EMBED_DIM)   #(2)\n",
    "look_ahead_mask = create_mask(seq_length=SEQ_LENGTH)  #(3)\n",
    "\n",
    "captions = decoder_block(features, captions, look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fca17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 19a\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #(1)\n",
    "        self.embedding = nn.Embedding(num_embeddings=VOCAB_SIZE,\n",
    "                                      embedding_dim=EMBED_DIM)\n",
    "\n",
    "        #(2)\n",
    "        self.sinusoidal_embedding = SinusoidalEmbedding()\n",
    "\n",
    "        #(3)\n",
    "        self.decoder_blocks = nn.ModuleList(DecoderBlock() for _ in range(NUM_DECODER_BLOCKS))\n",
    "\n",
    "        #(4)\n",
    "        self.linear = nn.Linear(in_features=EMBED_DIM, \n",
    "                                out_features=VOCAB_SIZE)\n",
    "        \n",
    "# Codeblock 19b\n",
    "    def forward(self, features, captions, attn_mask):  #(1)\n",
    "        #print(f\"features\\t\\t: {features.shape}\")\n",
    "        #print(f\"captions\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        captions = self.embedding(captions)  #(2)\n",
    "        #print(f\"after embedding\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        captions = captions + self.sinusoidal_embedding()  #(3)\n",
    "        #print(f\"after sin embed\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        for i, decoder_block in enumerate(self.decoder_blocks):\n",
    "            captions = decoder_block(features, captions, attn_mask)  #(4)\n",
    "            #print(f\"after decoder block #{i}\\t: {captions.shape}\")\n",
    "        \n",
    "        captions = self.linear(captions)  #(5)\n",
    "        #print(f\"after linear\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c07c8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\t\t: torch.Size([1, 576, 768])\n",
      "captions\t\t: torch.Size([1, 30])\n",
      "after embedding\t\t: torch.Size([1, 30, 768])\n",
      "after sin embed\t\t: torch.Size([1, 30, 768])\n",
      "after decoder block #0\t: torch.Size([1, 30, 768])\n",
      "after decoder block #1\t: torch.Size([1, 30, 768])\n",
      "after decoder block #2\t: torch.Size([1, 30, 768])\n",
      "after decoder block #3\t: torch.Size([1, 30, 768])\n",
      "after linear\t\t: torch.Size([1, 30, 10000])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 20\n",
    "decoder = Decoder()\n",
    "\n",
    "features = torch.randn(BATCH_SIZE, NUM_PATCHES, EMBED_DIM)\n",
    "captions = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LENGTH))  #(1)\n",
    "\n",
    "captions = decoder(features, captions, look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a6e81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 21\n",
    "class DecoderTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=VOCAB_SIZE,\n",
    "                                      embedding_dim=EMBED_DIM)\n",
    "        \n",
    "        self.sinusoidal_embedding = SinusoidalEmbedding()\n",
    "        \n",
    "        #(1)\n",
    "        decoder_block = nn.TransformerDecoderLayer(d_model=EMBED_DIM, \n",
    "                                                   nhead=NUM_HEADS, \n",
    "                                                   dim_feedforward=HIDDEN_DIM, \n",
    "                                                   dropout=DROP_PROB, \n",
    "                                                   batch_first=True)\n",
    "        \n",
    "        #(2)\n",
    "        self.decoder_blocks = nn.TransformerDecoder(decoder_layer=decoder_block, \n",
    "                                                    num_layers=NUM_DECODER_BLOCKS)\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=EMBED_DIM, \n",
    "                                out_features=VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, features, captions, tgt_mask):\n",
    "        #print(f\"features\\t\\t: {features.shape}\")\n",
    "        #print(f\"captions\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        captions = self.embedding(captions)\n",
    "        #print(f\"after embedding\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        captions = captions + self.sinusoidal_embedding()\n",
    "        #print(f\"after sin embed\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        #(3)\n",
    "        captions = self.decoder_blocks(tgt=captions, \n",
    "                                       memory=features, \n",
    "                                       tgt_mask=tgt_mask)\n",
    "        #print(f\"after decoder blocks\\t: {captions.shape}\")\n",
    "        \n",
    "        captions = self.linear(captions)\n",
    "        #print(f\"after linear\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1575127c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\t\t: torch.Size([1, 576, 768])\n",
      "captions\t\t: torch.Size([1, 30])\n",
      "after embedding\t\t: torch.Size([1, 30, 768])\n",
      "after sin embed\t\t: torch.Size([1, 30, 768])\n",
      "after decoder blocks\t: torch.Size([1, 30, 768])\n",
      "after linear\t\t: torch.Size([1, 30, 10000])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 22\n",
    "decoder_torch = DecoderTorch()\n",
    "\n",
    "features = torch.randn(BATCH_SIZE, NUM_PATCHES, EMBED_DIM)\n",
    "captions = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LENGTH))\n",
    "\n",
    "captions = decoder_torch(features, captions, look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23c2a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 23\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()  #EncoderTorch()  #(1)\n",
    "        self.decoder = Decoder()  #DecoderTorch()  #(2)\n",
    "        \n",
    "    def forward(self, images, captions, look_ahead_mask):  #(3)\n",
    "        print(f\"images\\t\\t\\t: {images.shape}\")\n",
    "        print(f\"captions\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        features = self.encoder(images)\n",
    "        print(f\"after encoder\\t\\t: {features.shape}\")\n",
    "        \n",
    "        captions = self.decoder(features, captions, look_ahead_mask)\n",
    "        print(f\"after decoder\\t\\t: {captions.shape}\")\n",
    "        \n",
    "        return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca1fa5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\t\t\t: torch.Size([1, 3, 384, 384])\n",
      "captions\t\t: torch.Size([1, 30])\n",
      "after encoder\t\t: torch.Size([1, 576, 768])\n",
      "after decoder\t\t: torch.Size([1, 30, 10000])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 24\n",
    "encoder_decoder = EncoderDecoder()\n",
    "\n",
    "images = torch.randn(BATCH_SIZE, IN_CHANNELS, IMAGE_SIZE, IMAGE_SIZE)  #(1)\n",
    "captions = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LENGTH))  #(2)\n",
    "\n",
    "captions = encoder_decoder(images, captions, look_ahead_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
