{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9b1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cba27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 2\n",
    "WIDTH_MULTIPLIER = 1.0\n",
    "INPUT_RESOLUTION = 224\n",
    "NUM_CLASSES      = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bebab1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 3\n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, num_channels, r):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.global_pooling = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "        self.fc0 = nn.Linear(in_features=num_channels,\n",
    "                             out_features=num_channels//r, \n",
    "                             bias=False)\n",
    "        self.relu6 = nn.ReLU6()\n",
    "        self.fc1 = nn.Linear(in_features=num_channels//r,\n",
    "                             out_features=num_channels, \n",
    "                             bias=False)\n",
    "        self.hardsigmoid = nn.Hardsigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f'original\\t\\t: {x.size()}')\n",
    "        \n",
    "        squeezed = self.global_pooling(x)              #(1)\n",
    "        #print(f'after avgpool\\t\\t: {squeezed.size()}')\n",
    "        \n",
    "        squeezed = torch.flatten(squeezed, 1)\n",
    "        #print(f'after flatten\\t\\t: {squeezed.size()}')\n",
    "        \n",
    "        excited = self.fc0(squeezed)                   #(2)\n",
    "        #print(f'after fc0\\t\\t: {excited.size()}')\n",
    "        \n",
    "        excited = self.relu6(excited)\n",
    "        #print(f'after relu6\\t\\t: {excited.size()}')\n",
    "        \n",
    "        excited = self.fc1(excited)                    #(3)\n",
    "        #print(f'after fc1\\t\\t: {excited.size()}')\n",
    "        \n",
    "        excited = self.hardsigmoid(excited)            #(4)\n",
    "        #print(f'after hardsigmoid\\t: {excited.size()}')\n",
    "        \n",
    "        excited = excited[:, :, None, None]\n",
    "        #print(f'after reshape\\t\\t: {excited.size()}')\n",
    "        \n",
    "        scaled = x * excited                           #(5)\n",
    "        #print(f'after scaling\\t\\t: {scaled.size()}')\n",
    "        \n",
    "        return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b14b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\t\t: torch.Size([1, 512, 28, 28])\n",
      "after avgpool\t\t: torch.Size([1, 512, 1, 1])\n",
      "after flatten\t\t: torch.Size([1, 512])\n",
      "after fc0\t\t: torch.Size([1, 128])\n",
      "after relu6\t\t: torch.Size([1, 128])\n",
      "after fc1\t\t: torch.Size([1, 512])\n",
      "after hardsigmoid\t: torch.Size([1, 512])\n",
      "after reshape\t\t: torch.Size([1, 512, 1, 1])\n",
      "after scaling\t\t: torch.Size([1, 512, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 4\n",
    "semodule = SEModule(num_channels=512, r=4)\n",
    "x = torch.randn(1, 512, 28, 28)\n",
    "\n",
    "out = semodule(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "683850bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 5\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,             #(1)\n",
    "                 out_channels,            #(2)\n",
    "                 kernel_size,             #(3)\n",
    "                 stride,                  #(4)\n",
    "                 padding,                 #(5)\n",
    "                 groups=1,                #(6)\n",
    "                 batchnorm=True,          #(7)\n",
    "                 activation=nn.ReLU6()):  #(8)\n",
    "        super().__init__()\n",
    "        \n",
    "        bias = False if batchnorm else True    #(9)\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size, \n",
    "                              stride=stride, \n",
    "                              padding=padding, \n",
    "                              groups=groups,\n",
    "                              bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels) if batchnorm else nn.Identity()  #(10)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):    #(11)\n",
    "        #print(f'original\\t\\t: {x.size()}')\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        #print(f'after conv\\t\\t: {x.size()}')\n",
    "        \n",
    "        x = self.bn(x)\n",
    "        #print(f'after bn\\t\\t: {x.size()}')\n",
    "        \n",
    "        x = self.activation(x)\n",
    "        #print(f'after activation\\t: {x.size()}')\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c65111ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvBlock(\n",
      "  (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU6()\n",
      ")\n",
      "\n",
      "ConvBlock(\n",
      "  (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (bn): Identity()\n",
      "  (activation): Hardswish()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 6\n",
    "convblock1 = ConvBlock(in_channels=64, \n",
    "                       out_channels=128, \n",
    "                       kernel_size=3, \n",
    "                       stride=2, \n",
    "                       padding=1)\n",
    "\n",
    "convblock2 = ConvBlock(in_channels=64, \n",
    "                       out_channels=128, \n",
    "                       kernel_size=3, \n",
    "                       stride=2, \n",
    "                       padding=1, \n",
    "                       batchnorm=False,             #(1)\n",
    "                       activation=nn.Hardswish())   #(2)\n",
    "\n",
    "print(convblock1)\n",
    "print('')\n",
    "print(convblock2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd19d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 7a\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size, \n",
    "                 stride,\n",
    "                 padding,\n",
    "                 exp_size,     #(1)\n",
    "                 se,           #(2)\n",
    "                 activation):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add = in_channels == out_channels and stride == 1    #(3)\n",
    "\n",
    "        self.conv0 = ConvBlock(in_channels=in_channels,    #(4)\n",
    "                               out_channels=exp_size,    #(5)\n",
    "                               kernel_size=1,    #(6)\n",
    "                               stride=1, \n",
    "                               padding=0,\n",
    "                               activation=activation)\n",
    "                               \n",
    "        self.conv1 = ConvBlock(in_channels=exp_size,    #(7)\n",
    "                               out_channels=exp_size,    #(8)\n",
    "                               kernel_size=kernel_size,    #(9)\n",
    "                               stride=stride, \n",
    "                               padding=padding,\n",
    "                               groups=exp_size,    #(10)\n",
    "                               activation=activation)\n",
    "\n",
    "        self.semodule = SEModule(num_channels=exp_size, r=4) if se else nn.Identity()    #(11)\n",
    "\n",
    "        self.conv2 = ConvBlock(in_channels=exp_size,    #(12)\n",
    "                               out_channels=out_channels,    #(13)\n",
    "                               kernel_size=1,    #(14)\n",
    "                               stride=1, \n",
    "                               padding=0, \n",
    "                               activation=nn.Identity())    #(15)\n",
    "        \n",
    "    # Codeblock 7b\n",
    "    def forward(self, x):\n",
    "            residual = x\n",
    "            #print(f'original\\t\\t: {x.size()}')\n",
    "\n",
    "            x = self.conv0(x)\n",
    "            #print(f'after conv0\\t\\t: {x.size()}')\n",
    "\n",
    "            x = self.conv1(x)\n",
    "            #print(f'after conv1\\t\\t: {x.size()}')\n",
    "\n",
    "            x = self.semodule(x)\n",
    "            #print(f'after semodule\\t\\t: {x.size()}')\n",
    "\n",
    "            x = self.conv2(x)\n",
    "            #print(f'after conv2\\t\\t: {x.size()}')\n",
    "\n",
    "            if self.add:\n",
    "                x += residual\n",
    "                #print(f'after summation\\t\\t: {x.size()}')\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfbd64a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\t\t: torch.Size([1, 16, 112, 112])\n",
      "after conv0\t\t: torch.Size([1, 64, 112, 112])\n",
      "after conv1\t\t: torch.Size([1, 64, 56, 56])\n",
      "after semodule\t\t: torch.Size([1, 64, 56, 56])\n",
      "after conv2\t\t: torch.Size([1, 24, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 8\n",
    "bottleneck = Bottleneck(in_channels=16,\n",
    "                        out_channels=24,   #(1)\n",
    "                        kernel_size=3,     #(2)\n",
    "                        exp_size=64,       #(3)\n",
    "                        stride=2,          #(4)\n",
    "                        padding=1, \n",
    "                        se=False,          #(5)\n",
    "                        activation=nn.ReLU6())  #(6)\n",
    "\n",
    "x = torch.randn(1, 16, 112, 112)           #(7)\n",
    "out = bottleneck(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8d6e31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bottleneck(\n",
       "  (conv0): ConvBlock(\n",
       "    (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): ReLU6()\n",
       "  )\n",
       "  (conv1): ConvBlock(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): ReLU6()\n",
       "  )\n",
       "  (semodule): Identity()\n",
       "  (conv2): ConvBlock(\n",
       "    (conv): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codeblock 9\n",
    "bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa82897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bottleneck(\n",
       "  (conv0): ConvBlock(\n",
       "    (conv): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): ReLU6()\n",
       "  )\n",
       "  (conv1): ConvBlock(\n",
       "    (conv): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "    (bn): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): ReLU6()\n",
       "  )\n",
       "  (semodule): SEModule(\n",
       "    (global_pooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc0): Linear(in_features=72, out_features=18, bias=False)\n",
       "    (relu6): ReLU6()\n",
       "    (fc1): Linear(in_features=18, out_features=72, bias=False)\n",
       "    (hardsigmoid): Hardsigmoid()\n",
       "  )\n",
       "  (conv2): ConvBlock(\n",
       "    (conv): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codeblock 10\n",
    "bottleneck = Bottleneck(in_channels=24, \n",
    "                        out_channels=40, \n",
    "                        kernel_size=5, \n",
    "                        exp_size=72,\n",
    "                        stride=2, \n",
    "                        padding=2, \n",
    "                        se=True, \n",
    "                        activation=nn.ReLU6())\n",
    "\n",
    "bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8505777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 11\n",
    "HS = nn.Hardswish()\n",
    "RE = nn.ReLU6()\n",
    "\n",
    "BOTTLENECKS = [[16,  16,  3, 16,  False, RE, 1, 1], \n",
    "               [16,  24,  3, 64,  False, RE, 2, 1], \n",
    "               [24,  24,  3, 72,  False, RE, 1, 1], \n",
    "               [24,  40,  5, 72,  True,  RE, 2, 2], \n",
    "               [40,  40,  5, 120, True,  RE, 1, 2], \n",
    "               [40,  40,  5, 120, True,  RE, 1, 2], \n",
    "               [40,  80,  3, 240, False, HS, 2, 1], \n",
    "               [80,  80,  3, 200, False, HS, 1, 1], \n",
    "               [80,  80,  3, 184, False, HS, 1, 1], \n",
    "               [80,  80,  3, 184, False, HS, 1, 1], \n",
    "               [80,  112, 3, 480, True,  HS, 1, 1], \n",
    "               [112, 112, 3, 672, True,  HS, 1, 1], \n",
    "               [112, 160, 5, 672, True,  HS, 2, 2], \n",
    "               [160, 160, 5, 960, True,  HS, 1, 2], \n",
    "               [160, 160, 5, 960, True,  HS, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71e7373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codeblock 12a\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.first_conv = ConvBlock(in_channels=3,    #(1)\n",
    "                                    out_channels=int(WIDTH_MULTIPLIER*16),\n",
    "                                    kernel_size=3,\n",
    "                                    stride=2,\n",
    "                                    padding=1, \n",
    "                                    activation=nn.Hardswish())\n",
    "        \n",
    "        self.blocks = nn.ModuleList([])    #(2)\n",
    "        for config in BOTTLENECKS:         #(3)\n",
    "            in_channels, out_channels, kernel_size, exp_size, se, activation, stride, padding = config\n",
    "            self.blocks.append(Bottleneck(in_channels=int(WIDTH_MULTIPLIER*in_channels), \n",
    "                                          out_channels=int(WIDTH_MULTIPLIER*out_channels), \n",
    "                                          kernel_size=kernel_size, \n",
    "                                          exp_size=int(WIDTH_MULTIPLIER*exp_size), \n",
    "                                          stride=stride, \n",
    "                                          padding=padding, \n",
    "                                          se=se, \n",
    "                                          activation=activation))\n",
    "        \n",
    "        self.second_conv = ConvBlock(in_channels=int(WIDTH_MULTIPLIER*160), #(4)\n",
    "                                     out_channels=int(WIDTH_MULTIPLIER*960),\n",
    "                                     kernel_size=1,\n",
    "                                     stride=1,\n",
    "                                     padding=0, \n",
    "                                     activation=nn.Hardswish())\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))              #(5)\n",
    "        \n",
    "        self.third_conv = ConvBlock(in_channels=int(WIDTH_MULTIPLIER*960),  #(6)\n",
    "                                    out_channels=int(WIDTH_MULTIPLIER*1280),\n",
    "                                    kernel_size=1,\n",
    "                                    stride=1,\n",
    "                                    padding=0, \n",
    "                                    batchnorm=False,\n",
    "                                    activation=nn.Hardswish())\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.8)    #(7)\n",
    "        \n",
    "        self.output = ConvBlock(in_channels=int(WIDTH_MULTIPLIER*1280),     #(8)\n",
    "                                out_channels=int(NUM_CLASSES),              #(9)\n",
    "                                kernel_size=1,\n",
    "                                stride=1,\n",
    "                                padding=0, \n",
    "                                batchnorm=False,\n",
    "                                activation=nn.Identity())\n",
    "        \n",
    "# Codeblock 12b\n",
    "    def forward(self, x):\n",
    "        print(f'original\\t\\t: {x.size()}')\n",
    "\n",
    "        x = self.first_conv(x)\n",
    "        print(f'after first_conv\\t: {x.size()}')\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            print(f\"after bottleneck #{i}\\t: {x.shape}\")\n",
    "        \n",
    "        x = self.second_conv(x)\n",
    "        print(f'after second_conv\\t: {x.size()}')\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        print(f'after avgpool\\t\\t: {x.size()}')\n",
    "        \n",
    "        x = self.third_conv(x)\n",
    "        print(f'after third_conv\\t: {x.size()}')\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        print(f'after dropout\\t\\t: {x.size()}')\n",
    "        \n",
    "        x = self.output(x)\n",
    "        print(f'after output\\t\\t: {x.size()}')\n",
    "        \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        print(f'after flatten\\t\\t: {x.size()}')\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8247ab10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\t\t: torch.Size([1, 3, 224, 224])\n",
      "after first_conv\t: torch.Size([1, 16, 112, 112])\n",
      "after bottleneck #0\t: torch.Size([1, 16, 112, 112])\n",
      "after bottleneck #1\t: torch.Size([1, 24, 56, 56])\n",
      "after bottleneck #2\t: torch.Size([1, 24, 56, 56])\n",
      "after bottleneck #3\t: torch.Size([1, 40, 28, 28])\n",
      "after bottleneck #4\t: torch.Size([1, 40, 28, 28])\n",
      "after bottleneck #5\t: torch.Size([1, 40, 28, 28])\n",
      "after bottleneck #6\t: torch.Size([1, 80, 14, 14])\n",
      "after bottleneck #7\t: torch.Size([1, 80, 14, 14])\n",
      "after bottleneck #8\t: torch.Size([1, 80, 14, 14])\n",
      "after bottleneck #9\t: torch.Size([1, 80, 14, 14])\n",
      "after bottleneck #10\t: torch.Size([1, 112, 14, 14])\n",
      "after bottleneck #11\t: torch.Size([1, 112, 14, 14])\n",
      "after bottleneck #12\t: torch.Size([1, 160, 7, 7])\n",
      "after bottleneck #13\t: torch.Size([1, 160, 7, 7])\n",
      "after bottleneck #14\t: torch.Size([1, 160, 7, 7])\n",
      "after second_conv\t: torch.Size([1, 960, 7, 7])\n",
      "after avgpool\t\t: torch.Size([1, 960, 1, 1])\n",
      "after third_conv\t: torch.Size([1, 1280, 1, 1])\n",
      "after dropout\t\t: torch.Size([1, 1280, 1, 1])\n",
      "after output\t\t: torch.Size([1, 1000, 1, 1])\n",
      "after flatten\t\t: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Codeblock 13\n",
    "mobilenetv3 = MobileNetV3()\n",
    "\n",
    "x = torch.randn(1, 3, INPUT_RESOLUTION, INPUT_RESOLUTION)\n",
    "out = mobilenetv3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbd74a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5476416"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codeblock 14\n",
    "total_params = sum(p.numel() for p in mobilenetv3.parameters())\n",
    "total_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
